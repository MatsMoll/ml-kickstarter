version: '3'
services:
  mlflow-tracker:
    image: project-base
    build:
      context: .
      dockerfile: docker/Dockerfile
    command: ["mlflow", "server", "--backend-store-uri", "file:///app/mlflow-server", "--host", "0.0.0.0", "--port", "8000"]
    ports:
      - 7999:8000
    volumes:
      - ./src:/app/src
      - ./mlflow:/app/mlflow-server

  movie_review_is_negative:
    image: project-serve
    build:
      context: .
      dockerfile: docker/Dockerfile.serve
    command: ["mlflow", "models", "serve", "-m", "models:/movie_review_is_negative@champion", "--port", "8080", "--host", "0.0.0.0", "--no-conda", "--enable-mlserver"]
    ports:
      - 8081:8080
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow-tracker:8000
    volumes:
      - ./src:/app/src

  wine-model:
    image: project-serve
    build:
      context: .
      dockerfile: docker/Dockerfile.serve
    command: ["mlflow", "models", "serve", "-m", "models:/is_high_quality_wine@champion", "--port", "8080", "--host", "0.0.0.0", "--no-conda"]
    ports:
      - 8080:8080
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow-tracker:8000
    volumes:
      - ./src:/app/src
    extra_hosts:
      - host.docker.internal:host-gateway

  trainer:
    image: project-base
    build:
      context: .
      dockerfile: docker/Dockerfile
    command: ["python", "-m", "src.pipelines.train"]
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow-tracker:8000
    volumes:
      - ./src:/app/src
      - ./data:/app/data
    extra_hosts:
      - host.docker.internal:host-gateway

  aligned-catalog:
    image: project-catalog
    build:
      context: .
      dockerfile: docker/Dockerfile.catalog
    command: ["python", "-m", "streamlit", "run", "app.py", "--server.port", "8501", "--server.fileWatcherType", "poll"]
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow-tracker:8000
    volumes:
      - ./src:/app/src
      - ./data:/app/data
      - ./load_contracts.py:/app/custom_store.py
      - ../aligned-managed-catalog/:/app/
      - ../aligned/aligned:/usr/local/lib/python3.11/site-packages/aligned/
    ports:
      - 8503:8501
    extra_hosts:
      - host.docker.internal:host-gateway

  prefect-server:
    image: prefecthq/prefect:2-python3.10
    command: ["prefect", "server", "start", "--port=4200", "--host=0.0.0.0"]
    ports:
      - 4201:4200

  pipeline-worker:
    image: project-dev
    build:
      context: .
      dockerfile: docker/Dockerfile.dev
    command: ["python", "-m", "src.pipelines.available"]
    volumes:
      - ./src:/app/src
      - ./data:/app/data
      - ../aligned/aligned:/usr/local/lib/python3.11/site-packages/aligned/
    environment:
      - "PREFECT_API_URL=http://prefect-server:4200/api"
      - "MLFLOW_TRACKING_URI=http://mlflow-tracker:8000"
    extra_hosts:
      - host.docker.internal:host-gateway

  # ollama:
  #   container_name: ollama
  #   image: ollama/ollama:latest
  #   # Uncomment below to expose Ollama API outside the container stack
  #   pull_policy: always
  #   tty: true
  #   restart: unless-stopped
  #   volumes:
  #     - ./ollama:/root/.ollama
  #   ports:
  #     - 11434:11434
  #

  # ollama-webui:
  #   image: ghcr.io/open-webui/open-webui
  #   container_name: ollama-webui
  #   depends_on:
  #     - ollama
  #   ports:
  #     - 3000:8080
  #   environment:
  #     - "OLLAMA_API_BASE_URL=http://ollama:11434/api"
  #   extra_hosts:
  #     - host.docker.internal:host-gateway
  #   restart: unless-stopped
